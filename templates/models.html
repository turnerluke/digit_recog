{% extends "base.html" %}


	{% block title %}
		Models
	{% endblock %}

	{% block content %}
		<div class="container" class="col-md-7">
            <div>
                <h1>Model</h1>
                <p>
                    The model used is the LeNet-5 v2.0 convolutional neural network (CNN), originally presented in <a href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">LeCun et al., Gradient-Based Learning Applied to Document Recognition</a>.

                    <br>
                    <br>

                    The model was implemented in Keras as shown here:

                    <br><br>

                    <a href="https://colab.research.google.com/github/turnerluke/digit_recog/blob/main/models/LeNet_5_train.ipynb">
                        <img src="https://colab.research.google.com/assets/colab-badge.svg">
                    </a>

                </p>

                <hr>

                <h1>Convolutional Neural Network Structure</h1>
                <p>
                    The structure of LeNet-5 v2.0 is as follows:

                    <br>

                    <table class="table table-hover">
                      <thead class="thead-dark">
                      <tr>
                        <th scope="col">Layer (Type)</th>
                        <th scope="col">Output Shape</th>
                        <th scope="col">Parameters</th>
                      </tr>
                      </thead>

                        <tr>
                          <td>Convolution_1 (conv2d)</td>
                          <td>(None, 28, 28, 32)</td>
                          <td>832</td>
                        </tr>
                        <tr>
                          <td>Convolution_2 (conv2d)</td>
                          <td>(None, 24, 24, 32)</td>
                          <td>25600</td>
                        </tr>
                        <tr>
                          <td>Batchnorm_1 (batchnormalization)</td>
                          <td>(None, 24, 24, 32)</td>
                          <td>128</td>
                        </tr>
                        <tr>
                          <td>Activation_25 (activation)</td>
                          <td>(None, 24, 24, 32)</td>
                          <td>0</td>
                        </tr>
                        <tr>
                          <td>Max_pool_1 (maxpooling2d)</td>
                          <td>(None, 12, 12, 32)</td>
                          <td>0</td>
                        </tr>
                        <tr>
                          <td>Dropout_1 (dropout)</td>
                          <td>(None, 12, 12, 32)</td>
                          <td>0</td>
                        </tr>
                        <tr>
                          <td>Convolution_3 (conv2d)</td>
                          <td>(None, 10, 10, 64)</td>
                          <td>18496</td>
                        </tr>
                        <tr>
                          <td>Convolution_4 (conv2d)</td>
                          <td>(None, 8, 8, 64 )</td>
                          <td>36864</td>
                        </tr>
                        <tr>
                          <td>Batchnorm_2 (batchnormalization)</td>
                          <td>(None, 8, 8, 64)</td>
                          <td>256</td>
                        </tr>
                        <tr>
                          <td>Activation_26 (activation)</td>
                          <td>(None, 8, 8, 64)</td>
                          <td>0</td>
                        </tr>
                        <tr>
                          <td>Max_pool_2 (maxpooling2d)</td>
                          <td>(None, 4, 4, 64)</td>
                          <td>0</td>
                        </tr>
                        <tr>
                          <td>Dropout_2 (dropout)</td>
                          <td>(None, 4, 4, 64)</td>
                          <td>0</td>
                        </tr>
                        <tr>
                          <td>Flatten (flatten)</td>
                          <td>(None, 1024)</td>
                          <td>0</td>
                        </tr>
                        <tr>
                          <td>Fully_connected_1 (dense)</td>
                          <td>(None, 256)</td>
                          <td>262144</td>
                        </tr>
                        <tr>
                          <td>Batchnorm_3 (batchnormalization)</td>
                          <td>(None, 256)</td>
                          <td>1024</td>
                        </tr>
                        <tr>
                          <td>Activation_27 (activation)</td>
                          <td>(None, 256)</td>
                          <td>0</td>
                        </tr>
                        <tr>
                          <td>Fully_connected_2 (dense)</td>
                          <td>(None, 128)</td>
                          <td>32768</td>
                        </tr>
                        <tr>
                          <td>Batchnorm_4 (batchnormalization)</td>
                          <td>(None, 128)</td>
                          <td>512</td>
                        </tr>
                        <tr>
                          <td>Activation_28 (activation)</td>
                          <td>(None, 128)</td>
                          <td>0</td>
                        </tr>
                        <tr>
                          <td>Fully_connected_3 (dense)</td>
                          <td>(None, 84)</td>
                          <td>10752</td>
                        </tr>
                        <tr>
                          <td>Batchnorm_5 (batchnormalization)</td>
                          <td>(None, 84)</td>
                          <td>336</td>
                        </tr>
                        <tr>
                          <td>Activation_29 (activation)</td>
                          <td>(None, 84)</td>
                          <td>0</td>
                        </tr>
                        <tr>
                          <td>Dropout_3 (dropout)</td>
                          <td>(None, 84)</td>
                          <td>0</td>
                        </tr>
                        <tr>
                          <td>Output (dense)</td>
                          <td>(None, 10)</td>
                          <td>850</td>
                        </tr>


                    </table>

                </p>

                <img src = "/static/LeNet_5_v2_Vis.png" alt = "LeNet 5 v2 Structure" width="95%"> </img>
                <br>
                <b>Figure 1:</b> <i>Model Structure (Created with <a href="http://alexlenail.me/NN-SVG/LeNet.html">NN-SVG</a>)</i>.

                <br>
                <br>

                <hr>
                <h1>Data</h1>
                <p>
                    The Modified National Institute of Standards and Technology (MNIST) dataset of handwritten digits was used as the training and validation datasets for this model. The dataset was obtained from the keras.datasets module as follows:
                    <br>
                    <code>
                        (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()
                    </code>
                    <br>

                    This dataset consists of 60,000 training and 10,000 testing digits and their corresponding labels.

                    The data is normalized to have a mean value of 0 and standard deviaiton of 1.

                    Data augmentation is performed to prevent overfitting of the dataset, using the following random operations:
                    <ul>
                        <li>10 degree rotations</li>
                        <li>10% zoom</li>
                        <li>10% horizontal shifts</li>
                        <li>10% vertical shifts</li>
                    </ul>

                </p>

                <h1>Training</h1>
                <p>
                    The model was trained for 30 epochs, as originally performed by LeCun et al. The loss function was categorical cross entropy, and the accuracy metric was utilized to quantify performance.
                </p>

                <h1>Model Performance</h1>
                <p>
                    This model achieved a final accuracy of 99.63% on the test set, after 30 epochs of training.

                    <img src = "static/training.png" alt = "Training Progress" width = "60%">
                    <br>
                    <b>Figure 2:</b> <i>Training and Validation Loss and Accuracy Versus Training Epoch</a></i>.

                    <br>

                </p>

                <h3>Confusion Matrix</h3>

                <img src = "/static/cm.png" alt = "Confusion Matrix LeNet-5 v2 on MNIST" width="60%"> </img>
                <br>
                <b>Figure 3:</b> <i>Confusion Matrix on the MNIST Dataset</a></i>.

                <br>

                <h3>Debugging</h3>

                <p>
                    If your digit sketches are not being properly predicted by the model, try drawing them to take up a majority of the space on the canvas.
                    Please note, that LeNet-5 v2.0 is exceptional at recognizing the MNIST dataset, so to ensure a similar performance on your sketches, they should closely match the data.

                    <br>

                    <img src = "/static/digits.png" alt = "MNIST Sample" width="60%">
                    <br>
                    <b>Figure 4:</b> <i>A Subset of the MNIST Dataset Digits</i>.
                </p>


            </div>
        </div>


	{% endblock %}